lines(cbind(newx,concrete_pred[,2]),col="green",lty=2)
lines(cbind(newx,concrete_pred[,3]),col="green",lty=2)
#This would mean that there is no linear relationship between the two variables.
#The p-value is incredibly small, indicating that there is an extremely high likelihood that a
#great deal of the strength of concrete can be explained by the amount of cement in the mixture.
#The confidence interval for mean observation is extremely narrow.
#The prediction interval for given values, however, is extremely wide,
#indicating that it will be hard to use this model to predict strength given a specific level of cement.
#there might be other variables that could explain the concrete the strength
fittest <- lm(data = training_data,compressive_strength ~ cement+blast_furnace_slag+fly_ash+water+superplasticizer+
coarse_aggregate+fine_aggregate+age)
summary(fittest)
#determining which variables are orthogonal
#remove one variable - fine_aggregate - because its p-value is larger.
#then fiting linear regression
fittest2<-update(fittest,~.-fine_aggregate)
summary(fittest2)
AIC(fittest2)
BIC(fittest2)
#now it can seen that coarse_aggregate is above the p-value threshold of .05, so we remove it - coarse_aggregate
fittest3<-update(fittest2,~.-coarse_aggregate)
summary(fittest3)
AIC(fittest3)
BIC(fittest3)
vif(fittest3)
train_pred <- predict(fittest3,testing_data)
RMSE(train_pred,testing_data$compressive_strength)
# this does not change the model now and R-Squared value decreases so  it can be still kept, but for now I am removing it
#### now all variables have a significant relationship with strength
#level=.95,interval="confidence")
library("datasets")
#residuals vs predicted values Plot
?plot
par(mfrow= c(2,2))
plot(fittest3)
influence.measures(fittest3)
library("car")
par(mfrow= c(1,1))
influencePlot(fittest3)
ncvTest(fittest3)
#The test have a p-value less that a significance level of 0.05,
#therefore we can reject the null hypothesis that the variance of the residuals
#is constant and infer that heteroscedasticity is indeed present,
#thereby confirming our graphical inference.
rstandard(fittest3)
par(mfrow = c(1,1))
plot(fittest3$fitted.values,rstandard(fittest3))
plot(fittest3$fitted.values ~ fittest3$residuals,xlab= "Residuals", ylab= "Predicted_Values",
main = "residuals vs predicted value Plot")
#leverage
plot(hatvalues(fittest3))
hatvalues(fittest3)[hatvalues(fittest)>0.2]
#cooksdistance
plot(cooks.distance(fittest3),rstudent(fittest3))
influence.measures(fittest3)
#checking varience correlation factor
vif(fittest3)
#----------------------------------------------------------------------------------------------------------------------------
housingData <- read.csv("housingData.csv")
myfun<-function(x) mean(is.na(x))
apply(housingData,2,myfun) #Getting percentage of missing values in each variable
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
#Dealing with missing vaues
housing <- housingData
#let us use mode imputation method for missing variables using mode imputation, except for some variables
housing[is.na(housing$MasVnrType), "MasVnrType"] <- getmode(housing$MasVnrType)
housing[is.na(housing$MasVnrArea), "MasVnrArea"] <- getmode(housingData$MasVnrArea)
housing[is.na(housing$BsmtCond), "BsmtCond"] <- getmode(housingData$BsmtCond)
housing[is.na(housing$BsmtQual), "BsmtQual"] <- getmode(housingData$BsmtQual)
housing[is.na(housing$BsmtExposure), "BsmtExposure"] <- getmode(housingData$BsmtExposure)
housing[is.na(housing$BsmtFinType1), "BsmtFinType1"] <- getmode(housingData$BsmtFinType1)
housing[is.na(housing$BsmtFinType2), "BsmtFinType2"] <- getmode(housingData$BsmtFinType2)
housing[is.na(housing$Electrical), "Electrical"] <- getmode(housingData$Electrical)
housing[is.na(housing$GarageType), "GarageType"] <- getmode(housingData$GarageType)
housing[is.na(housing$GarageQual), "GarageQual"] <- getmode(housingData$GarageQual)
housing[is.na(housing$GarageCond), "GarageCond"] <- getmode(housingData$GarageCond)
housing[is.na(housing$GarageFinish), "GarageFinish"] <- getmode(housingData$GarageFinish)
#Removing following variables having most  NA values as they doesn't affect SalePrice
ggplot(housing, aes(housing$Alley, housing$SalePrice)) + geom_point() + geom_boxplot()
ggplot(housing, aes(housing$PoolQC, housing$SalePrice)) + geom_point() + geom_boxplot()
ggplot(housing, aes(housing$MiscFeature, housing$SalePrice)) + geom_point() + geom_boxplot()
ggplot(housing, aes(housing$Fence, housing$SalePrice)) + geom_point() + geom_boxplot()
housing$Alley <- NULL
housing$PoolQC <- NULL
housing$MiscFeature <- NULL
housing$Fence <- NULL
#assigning missing values in GarageYrBlt to YearBuilt as most garages are built in the same year as yearbuilt
ggplot(housing, aes(housing$YearBuilt, housing$GarageYrBlt)) + geom_point()
housing$GarageYrBlt[is.na(housing$GarageYrBlt)] <- housing$YearBuilt[is.na(housing$GarageYrBlt)]
#LotFrontage is almost similar to houses in the same Neighbourhood. So , we'll assign median LotFrontage in that Neighborhood to missing values
housing$LotFrontage[is.na(housing$LotFrontage)] <- median(housing$LotFrontage[housing$Neighborhood[is.na(housing$LotFrontage)]], na.rm = TRUE)
par(mfrow = c(1,2))
hist(housing$SalePrice) #SalePrice is skewed. so, we ransform it into log(1+SalePrice)
housing$SalePrice <- log(1+housing$SalePrice)  #To normally distribute the salePrice.
hist(housing$SalePrice, xlab = "log(1+SalePrice)")
dev.off()
#Similarly, we'll transform LotArea and LotFrontage into their log transformations using symboc function.
plot(housing$LotArea, housing$SalePrice)
hist(housing$LotArea)
symbox(housing$LotArea, data=housing, powers=c(3,2,1,0,-0.5,-1,-2)) #Use log transformation on LotArea
housing$LotArea <- log(housing$LotArea)
hist(housing$LotArea)
plot(housing$LotFrontage, housing$SalePrice)
hist(housing$LotFrontage)
symbox(housing$LotFrontage, data=housing, powers=c(3,2,1,0,-0.5,-1,-2)) #Use log transformation on LotArea
housing$LotFrontage <- log(housing$LotArea)
hist(housing$LotFrontage)
nums <- unlist(lapply(housing, is.numeric))
housing_corr <- housing[,nums]
corr_h <- cor(housing_corr)
corrplot(corr_h, method = "square")
#1. 'GarageCars' and 'GarageArea' show high correlation. Also, both of them seem to have a similar correlation with 'SalePrice'.
# This is a clear case of multicollinearity. Thus we can remove one of them.
# 2.TotalBsmtSF' and '1stFlrSF' show high correlation again indicating multicollinearity. We should thus remove one of these.
#But TotalBsmtSF and SalePrice are highly are highly correlated. So, we retain TotalBsmtSF as it may help in predicting SalePrice
#3.GarageYrBuilt and YearBuilt have high correlation and also we have alloted 5.3% of missing GarageYrBuilt to YearBuilt.
#YearBuilt is more related to SalePrice. So, we reatain SalePrice.
#4.GrLivArea, TotRmsAbvGrd are highly correlated. GrLiv area is more related to SalePrice. So, remove TotRmsAbvGrd.
#5.LotArea and LotFrontage are highly correlated. So, we can remove one of them.
#6.Also, other variables having correlation with SalePrice are OverallQual, FullBath, KitchenAbvGr. So, we'll keep them.
housing$Id <-NULL
housing$GarageArea <- NULL
housing$GarageYrBlt <- NULL
housing$TotRmsAbvGrd <- NULL
plot(housing$MSSubClass, housing$LotArea)
str(housing$MSSubClass)
housing$age <- housing$YearBuilt - housing$YrSold
housing$Remod_age <- housing$YearRemodAdd - housing$YrSold
#age and Remod age of houses are haveing similar impact on SalePrics as YearBuilt. Spo, we'll remove them.
housing[,c("age", "Remod_age")] <- NULL
par(mfrow = c(1,2))
plot(housing$X1stFlrSF, housing$SalePrice)
plot(housing$X2ndFlrSF, housing$SalePrice)
#From these two graphs we can see that 1stFlrSF has reasonably normal distribution, whereas 2ndFlrSF has more zero.
#These values conflict the regression line, resulting in poor estimator.So, we add these two variables to TotalSF
housing$TotalSF <- housing$X1stFlrSF + housing$X2ndFlrSF
ggplot(housing, aes(housing$TotalSF, housing$SalePrice)) + geom_point()
housing[,c('X1stFlrSF', 'X2ndFlrSF')] <- list(NULL)
plot(housing$FullBath, housing$SalePrice)
plot(housing$HalfBath, housing$SalePrice)
#We create new feature TotBath addding FullBath, HalfBath, BsmtFullBath, BsmtHalfBath.
housing$TotalBath <- housing$FullBath + 0.5*housing$HalfBath + housing$BsmtFullBath + 0.5*housing$BsmtHalfBath
ggplot(housing, aes(housing$TotalBath, housing$SalePrice)) + geom_point()
dev.off()
housing[,c('FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath')] <- NULL
#If we plot correlattion plot again we find GrLivArea and TotalSf are correlated and both are similarly related to SalePrice.
#So, we are removing TotalSf to prevent multicollinearity.
#Also, totalBath seems to be more related to SalePrice. So, we retain this removing FullBath and HalfBath.
nums <- unlist(lapply(housing, is.numeric))
housing_corr <- housing[,nums]
corr_h <- cor(housing_corr)
corrplot(corr_h, method = "square")
housing$TotalSF <-NULL
#return information on all features regarding zero variance and near-zero variance
nearZeroVar(housingData, saveMetrics = T)
#returns the column number of features which might be worth checking out as not useful
(colIDs<-nearZeroVar(housingData))
#take a quick look at the columns
head(housingData[,colIDs])
#look at factor levels and counts for each
apply(housingData[,colIDs],2,table)
#Now,based on above results let us see each variable and do EDA.
ggplot(housing, aes(x= housing$GarageType)) + geom_bar()
#Most of the values for GarageType feature are Attchd, Detchd & Builtin. So, we can create new feature for Attch garage(Attch & Buitin).
#Rest all are in Detchd
housing$GarageAttchd <- ifelse(housing$GarageType == "Attchd" | housing$GarageType == "BuiltIn", 1, 0)
housing$GarageAttchd <- as.integer(housing$GarageAttchd)
housing$GarageDetchd <- (housing$GarageType == 'Detchd') * 1
housing$GarageType <- NULL
ggplot(housing, aes(x= housing$LandContour)) + geom_bar()
#Most of the LandContours are Lvl. So, we can create new feature LandLvl to represent leaveled and remaining doesn't.
housing$LandLvl <- (housing$LandContour == 'Lvl') * 1
housing$LandContour <- NULL
ggplot(housing, aes(x= housing$LandSlope)) + geom_bar()
#Most of the LandSlope are Gtl. So, we can create new feature LandSlopeGtl to represent gentled slope and remaining doesn't.
housing$LandSlopeGtl <- (housing$LandSlope == 'Gtl') * 1
housing$LandSlope <- NULL
ggplot(housing, aes(x= housing$BsmtCond)) + geom_bar()
ggplot(housing, aes(x = housing$BsmtCond, y= housing$SalePrice)) + geom_point() + geom_boxplot()
#From above graphs we can combine ABoveAvg and Avg to Good and others to belowavg
housing$BsmtCondGood <- ifelse(housing$BsmtCond == "AboveAvg" | housing$BsmtCond == "Avg", 1, 0)
housing$BsmtCondGood <- as.integer(housing$BsmtCondGood)
housing$BsmtCond <- NULL
ggplot(housing, aes(x= housing$BsmtFinType2)) + geom_bar()
#From above graphs we can make new feature with BsmtFinType2 uniform or nor
housing$BsmtFinType2Unf <- (housing$BsmtFinType2 == 'Unf') * 1
housing$BsmtFinType2 <- NULL
ggplot(housing, aes(x= housing$Heating)) + geom_bar()
ggplot(housing, aes(x = housing$Heating, y= housing$SalePrice)) + geom_point() + geom_boxplot()
#one hot encoding of heating GasA or not.
housing$heatingGas <- (housing$Heating == 'GasA') * 1
housing$Heating <- NULL
ggplot(housing, aes(x= housing$LowQualFinSF)) + geom_bar()
#We'll create new feature with NoLowQualFinSF with zero LowQualFinSF or not.
housing$NoLowQualFinSF <- (housing$LowQualFinSF == '0') * 1
housing$LowQualFinSF <- NULL
ggplot(housing, aes(x= housing$KitchenAbvGr)) + geom_bar()
ggplot(housing, aes(x = housing$KitchenAbvGr, y= housing$SalePrice)) + geom_point()
#KitchenAbvGr doesn't seem to affect the SalePrice. So, we'll removre it
housing$KitchenAbvGr <- NULL
ggplot(housing, aes(x= housing$Functional)) + geom_bar()
#one hot encoding of Functional Typ or not.
housing$FunctionalTyp <- (housing$Functional == 'Typ') * 1
housing$Functional <- NULL
ggplot(housing, aes(x= housing$GarageQual)) + geom_bar()
ggplot(housing, aes(x = housing$GarageQual, y= housing$SalePrice)) + geom_point() + geom_boxplot()
#From above graphs we can combine ABoveAvg and Avg to Good and others to belowavg
housing$GarageQualGood <- ifelse(housing$GarageQual == "AboveAvg" | housing$GarageQual == "Avg", 1, 0)
housing$GarageQualGood <- as.integer(housing$GarageQualGood)
housing$GarageQual <- NULL
ggplot(housing, aes(x= housing$GarageCond)) + geom_bar()
ggplot(housing, aes(x = housing$GarageCond, y= housing$SalePrice)) + geom_point() + geom_boxplot()
#From above graphs we can combine ABoveAvg and Avg to Good and others to belowavg
housing$GarageCondGood <- ifelse(housing$GarageCond == "AboveAvg" | housing$GarageCond == "Avg", 1, 0)
housing$GarageCondGood <- as.integer(housing$GarageCondGood)
housing$GarageCond <- NULL
ggplot(housing, aes(x= housing$PoolArea)) + geom_bar()
ggplot(housing, aes(x = housing$PoolArea, y= housing$SalePrice)) + geom_point() + geom_boxplot()
#Removing PoolArea
housing$PoolArea <- NULL
ggplot(housing, aes(x= housing$MiscVal)) + geom_bar()
ggplot(housing, aes(x = housing$MiscVal, y= housing$SalePrice)) + geom_point()
#Removing MiscVal
housing$MiscVal <- NULL
ggplot(housing, aes(x= housing$SaleType)) + geom_bar()
ggplot(housing, aes(x = housing$SaleType, y= housing$SalePrice)) + geom_boxplot()
#Removing SaleType
housing$SaleType <- NULL
housing$FireplaceQu <- NULL
housing[,c(" BsmtFinSF1", "BsmtFinSF2")] <- list(NULL)
housing_train <- housing[1:800,]
housing_test <- housing[801:1000,]
fit1 <- lm(SalePrice~., data = housing_train)
summary(fit1)
AIC(fit1) # -1564.451
BIC(fit1) # -978.875
vif(fit1)
RSS <- c(crossprod(fit1$residuals))
MSE <- RSS / length(fit1$residuals) #Mean squared error:
RMSE <- sqrt(MSE) #0.07785005 #Root Mean Squared Error
RMSE
fit2 <- stepAIC(fit1, direction = "both")
summary(fit2)
AIC(fit2) #-1602.054
BIC(fit2) #-1189.808
vif(fit2)
RSS <- c(crossprod(fit2$residuals))
MSE <- RSS / length(fit2$residuals) #Mean squared error:
RMSE <- sqrt(MSE) # 0.07964134 #Root Mean Squared Error
RMSE
predict_housing <- predict(fit2, housing_test)
par(mfrow = c(2,2))
plot(fit2)
#The first plot clearly shows that residuals of our model are spread equally along the horizontal line, indicating that our model do not have non-linear relationship. There are few variables which are far from the horizontal line.
#The QQ Plot indicates residuals are clearly normally distributed as they have all the values held on the dashed line.
#The Standardized and fitted values plot shows few of the variables are not transformed but many of them are falls I straight line.
#Standardized residuals vs leverage indicates that 402 observation is outside cooks distance.  617 and 754 have high leverage.
dev.off()
plot(cooks.distance(fit2), rstudent(fit2))
influencePlot(fit2)
ncvTest(fit2)
plot(fit2$fitted.values, fit2$residuals)
fit2 <- stepAIC(fit1, direction = "both")
housing_train <- housing[1:800,]
housing_test <- housing[801:1000,]
fit1 <- lm(SalePrice~., data = housing_train)
summary(fit1)
AIC(fit1) # -1564.451
BIC(fit1) # -978.875
vif(fit1)
RSS <- c(crossprod(fit1$residuals))
MSE <- RSS / length(fit1$residuals) #Mean squared error:
RMSE <- sqrt(MSE) #0.07785005 #Root Mean Squared Error
RMSE
fit2 <- stepAIC(fit1, direction = "both")
summary(fit2)
AIC(fit2) #-1602.054
BIC(fit2) #-1189.808
vif(fit2)
RSS <- c(crossprod(fit2$residuals))
MSE <- RSS / length(fit2$residuals) #Mean squared error:
RMSE <- sqrt(MSE) # 0.07964134 #Root Mean Squared Error
RMSE
predict_housing <- predict(fit2, housing_test)
par(mfrow = c(2,2))
plot(fit2)
#The first plot clearly shows that residuals of our model are spread equally along the horizontal line, indicating that our model do not have non-linear relationship. There are few variables which are far from the horizontal line.
#The QQ Plot indicates residuals are clearly normally distributed as they have all the values held on the dashed line.
#The Standardized and fitted values plot shows few of the variables are not transformed but many of them are falls I straight line.
#Standardized residuals vs leverage indicates that 402 observation is outside cooks distance.  617 and 754 have high leverage.
dev.off()
plot(cooks.distance(fit2), rstudent(fit2))
influencePlot(fit2)
ncvTest(fit2)
plot(fit2$fitted.values, fit2$residuals)
setwd("C:/Users/dvsnv/Desktop/OU_Material/IDA/Homework/Homework_6/SAGI_HW6")
library(tidyverse)
library(Amelia)
library(caret)
library(MASS) #For boxcox transformations
library(corrplot)
library(psych)
library(corrgram)
library(mlbench)
library(e1071) # For finding skewness
library(car) #For symbox func
library(pls)
library(glmnet) #For ridge regression
library(elasticnet)
library(caret)
library(hqreg)
library(xgboost)
library(gbm)
library(nnet)
crimeTrain <- read.csv("crimeTrain.csv")
crimeTest <- read.csv("crimeTest.csv")
View(crimeTest)
cTest <- read.csv("crimeTest.csv")
crimeTest$ViolentCrimesPerPop <- NA
crimeTest$Id <- NULL
crimeTrain <- rbind(crimeTrain,crimeTest)
Train <- crimeTrain
View(crimeTrain)
myfun<-function(x) mean(is.na(x))
apply(crimeTrain,2,myfun) #Getting percentage of missing values in each variable
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
plot(Train$FTPoliceFieldPerPop, Train$ViolentCrimesPerPop)
plot(Train$county, Train$ViolentCrimesPerPop) #Target variable doesn't seem to be affected by county
str(Train$county)
unique(Train$communityname) #has 1135 unique values. So, can be removed.
unique(Train$county)
crimeTrain$county <- NULL
crimeTrain <- crimeTrain[,c(1:99)]
crimeTrain$LandArea <- Train$LandArea
crimeTrain$PopDens <- Train$PopDens
crimeTrain$PctUsePubTrans <- Train$PctUsePubTrans
crimeTrain$ViolentCrimesPerPop <- Train$ViolentCrimesPerPop
crimeTrain$communityname <- NULL
crimeTrain$X <- NULL
#OtherPerCap
sum(is.na(crimeTrain$OtherPerCap))
str(crimeTrain$OtherPerCap)
hist(crimeTrain$OtherPerCap)
unique(crimeTrain$OtherPerCap)
crimeTrain$OtherPerCap[is.na(crimeTrain$OtherPerCap)] <- getmode(crimeTrain$OtherPerCap)
#Replacing one missing value with mode value
summary(crimeTrain$householdsize)
summary(crimeTrain$PersPerOccupHous)
#As these both variables mean the same and also have similarl value, householdsize is removed
crimeTrain$householdsize <- NULL
hist(crimeTrain$ViolentCrimesPerPop)
hist(crimeTrain$pctUrban)
?apply
skew <- apply(crimeTrain, 2, skewness)
print(skew[skew > 3])
#Let us look into these variables with high skewness and transform them
str(crimeTrain$population)
crimeTrain$population <- crimeTrain$population + 0.01
symbox(crimeTrain$population, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$population <- crimeTrain$population^(-0.5)
crimeTrain$numbUrban <- crimeTrain$numbUrban + 0.01
symbox(crimeTrain$numbUrban, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$numbUrban <- log(crimeTrain$numbUrban)
crimeTrain$NumUnderPov <- crimeTrain$NumUnderPov + 0.01
symbox(crimeTrain$NumUnderPov, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$NumUnderPov <- crimeTrain$NumUnderPov^(-0.5)
crimeTrain$NumIlleg <- crimeTrain$NumIlleg + 0.01
symbox(crimeTrain$NumIlleg, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$NumIlleg <- log(crimeTrain$NumIlleg)
crimeTrain$NumImmig <- crimeTrain$NumImmig + 0.01
symbox(crimeTrain$NumImmig, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$NumImmig <- crimeTrain$NumImmig^(-0.5)
crimeTrain$HousVacant <- crimeTrain$HousVacant + 0.01
symbox(crimeTrain$HousVacant, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$HousVacant <- crimeTrain$HousVacant^(-0.5)
crimeTrain$NumInShelters <- crimeTrain$NumInShelters + 0.01
symbox(crimeTrain$NumInShelters, data = crimeTrain, powers=c(3,2,1,0,-0.5,-1,-2))
crimeTrain$NumInShelters <- crimeTrain$NumInShelters^(-1)
crimeTrain$state <- as.factor(crimeTrain$state)
ggplot(crimeTrain, aes(x = crimeTrain$medIncome, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$medFamInc, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$perCapInc, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
#From the above two plots we can remoce medIncome and medFamInc
crimeTrain$medIncome <- NULL
crimeTrain$medFamInc <- NULL
crimeTrain$numbUrban <- NULL
crimeTrain$NumUnderPov <- NULL
ggplot(crimeTrain, aes(x = crimeTrain$PctLess9thGrade, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$PctNotHSGrad, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$PctBSorMore, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
crimeTrain$PctLess9thGrade <- NULL
ggplot(crimeTrain, aes(x = crimeTrain$PctUnemployed, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$PctEmploy, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
cor(crimeTrain$PctNotHSGrad, crimeTrain$PctUnemployed)
#From the above correlation value and plots, Unemployed and PctNotHSGrad are correlated and similarly affecting target variable.
#So, we'll remove PctNotHSGrad to avoid overfitting.
crimeTrain$PctNotHSGrad <- NULL
cor(crimeTrain$PctEmploy, crimeTrain$PctUnemployed)
crimeTrain$PctEmplManu <- NULL
crimeTrain$PctEmplProfServ <- NULL
cor(crimeTrain$MalePctDivorce, crimeTrain$FemalePctDiv)
cor(crimeTrain$MalePctDivorce, crimeTrain$TotalPctDiv)
#From above two we can remove MalePctDivorce and FemalePctDiv
crimeTrain$MalePctDivorce <- NULL
crimeTrain$FemalePctDiv <- NULL
crimeTrain$NumIlleg <- NULL
crimeTrain$PctKids2Par <- NULL
cor(crimeTrain$PctYoungKids2Par,crimeTrain$PctTeen2Par)
cor(crimeTrain$PctYoungKids2Par,crimeTrain$ViolentCrimesPerPop)
cor(crimeTrain$PctTeen2Par,crimeTrain$ViolentCrimesPerPop)
crimeTrain$PctTeen2Par <- NULL
crimeTrain$PctWorkMomYoungKids <- NULL
cor(crimeTrain$PctRecImmig10, crimeTrain$PctRecentImmig)
cor(crimeTrain$PctImmigRecent, crimeTrain$PctImmigRec10)
cor(crimeTrain$PctRecentImmig, crimeTrain$ViolentCrimesPerPop)
cor(crimeTrain$PctImmigRec10, crimeTrain$ViolentCrimesPerPop)
crimeTrain$PctRecImmig5 <- NULL
crimeTrain$PctRecImmig8 <- NULL
crimeTrain$PctImmigRec5 <- NULL
crimeTrain$PctImmigRec8 <- NULL
cor(crimeTrain$PctSpeakEnglOnly,crimeTrain$PctNotSpeakEnglWell)
crimeTrain$PctLargHouseOccup <- NULL
crimeTrain$PersPerOccupHous <- NULL
crimeTrain$PersPerOwnOccHous <- NULL
crimeTrain$PersPerRentOccHous <- NULL
crimeTrain$MedYrHousBuilt <- NULL
ggplot(crimeTrain, aes(x = crimeTrain$MedOwnCostPctInc, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
ggplot(crimeTrain, aes(x = crimeTrain$MedOwnCostPctIncNoMtg, y = crimeTrain$ViolentCrimesPerPop)) + geom_point()
cor(crimeTrain$MedOwnCostPctInc, crimeTrain$MedOwnCostPctIncNoMtg)
cor(crimeTrain$MedOwnCostPctInc, crimeTrain$ViolentCrimesPerPop)
cor(crimeTrain$ViolentCrimesPerPop, crimeTrain$MedOwnCostPctIncNoMtg)
cor(crimeTrain$OwnOccMedVal, crimeTrain$OwnOccHiQuart)
cor(crimeTrain$OwnOccMedVal, crimeTrain$OwnOccLowQuart)
cor(crimeTrain$RentMedian, crimeTrain$RentLowQ)
cor(crimeTrain$RentMedian, crimeTrain$RentHighQ)
crimeTrain$OwnOccHiQuart <- NULL
crimeTrain$OwnOccLowQuart <- NULL
crimeTrain$RentLowQ <- NULL
crimeTrain$RentHighQ <- NULL
crimeTrain$MedRent <- NULL
cor(crimeTrain$RentMedian, crimeTrain$MedRent)
cor(crimeTrain$PctSameCity85, crimeTrain$PctSameHouse85)
crimeTrain$PctSameHouse85 <- NULL
par(mar = rep(0,4))
nums <- unlist(lapply(crimeTrain, is.numeric))
crime_corr <- crimeTrain[,nums]
corr_c <- cor(crime_corr)
corrplot(corr_c, method = "square", tl.cex = 0.6, tl.offset = 0.4,tl.srt = 90, cl.ratio = 0.3)
#The following observations can be made from the above correlation plot.
#1. age12t21 and age12t29 have correlation. This may be due to overlapping in data. So, we,ll make age12t29 to age21t29
#2. age65up and pctWSocSec are highly correlated and pctWSocSec is more related to ViolentCrimesPerPop.
#SO, we'll remove age65up to avoid overfitting.
#3. perCapInc and whitePerCap are highly correlated and seems to be similarly correlated with other variables.
#So, wel
#4. PctBSorMore and PctOccupMgmtProf are highly correlated and PctOccupMgmtProf seems to be more related to other variables.
#So, we'll remove PctBSorMore.
#5. PctFam2Par and PctYoungKids2Par are highly correlated. PctYoungKids2Par is removed to avoid multicollinearity.
#6. PctPersOwnOccup and PctHousOwnOcc are highly correlated and are similarly related to other variables.
# So, we'll remove PctPersOwnOccup
#7. PctRecentImmig and PctForeignBorn are highly correlated. PctForeignBorn is removed to avoid multicollinearity.
#8. OwnOccMedVal and RentMedian are highly correlated. OwnOccMedVal is removed to avoid multicollinearity.
#9. Also, other variables having correlation with ViolentCrimesPerPop are racepctblack, racepctwhite,pctWInvInc, pctWPubAsst,PctPopUnderPov,
# PctUnemployed,PctFam2Par, PersPerOwnOccHous, HousVacant.
crimeTrain$agePct12t21 <- NULL
cor(crimeTrain$perCapInc, crimeTrain$whitePerCap)
crimeTrain$agePct65up <- NULL
crimeTrain$PctBSorMore <- NULL
crimeTrain$PctYoungKids2Par <- NULL
crimeTrain$PctPersOwnOccup <- NULL
crimeTrain$PctForeignBorn <- NULL
crimeTrain$OwnOccMedVal <- NULL
crimeTrain$agePct16t24 <- NULL
dev.off()
cor(crimeTrain$PctFam2Par, crimeTrain$PctYoungKids2Par)
cor(crimeTrain$PctFam2Par, crimeTrain$ViolentCrimesPerPop)
cor(crimeTrain$ViolentCrimesPerPop, crimeTrain$OwnOccMedVal)
cor(crimeTrain$RentMedian, crimeTrain$ViolentCrimesPerPop)
cor(crimeTrain$agePct12t21, crimeTrain$agePct16t24)
#######################################################  PLS ###########################################
my_full <- crimeTrain
crimeTrain1 <- crimeTrain[1:1200,]
crimeTest1 <- crimeTrain[1201:1850,]
View(crimeTest1)
View(crimeTrain1)
set.seed(100)
train_index <- sample(1:nrow(crimeTrain1), 1000)
crimeTrain_train <- crimeTrain1[train_index,]
crimeTrain_test <- crimeTrain1[-train_index,]
train_control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)  # 5 fold cross validation
grid2 <- expand.grid(C = seq(1,10,length=15))
svm_fit <- train(ViolentCrimesPerPop~., data = crimeTrain_train, method = "svmLinear",
preProc = c("center", "scale"), tuneLength = 10,
trControl = train_control)
print(svm_fit)
plot(svm_fit,
plotType = "scatter",
metric = "RMSE")
grid2 <- expand.grid(C = seq(1,10,length=15))
SVM_fit1 <- train(ViolentCrimesPerPop~.,
data=crimeTrain_train,
method="svmLinear",        #linear Kernal
preProc = c("center","scale"),
trControl=train_control,
tuneGrid = grid2)
SVM_fit1
plot(SVM_fit1)
